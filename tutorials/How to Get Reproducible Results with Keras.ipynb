{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "**What?** How to Get Reproducible Results with Keras\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Getting rid of the warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need to fix the seeder at all?\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Neural network algorithms are stochastic hence they make use of randomness.\n",
    "- Sources of randomness:\n",
    "    - Randomness in Initialization, such as weights.\n",
    "    - Randomness in Regularization, such as dropout.\n",
    "    - Randomness in Layers, such as word embedding.\n",
    "    - Randomness in Optimization, such as stochastic optimization.\n",
    "- The consequences is that the same network trained on the same data can produce different results.\n",
    "- The random initialization allows the network to learn a good approximation for the function being learned. This is done by design.\n",
    "- **Nevertheless**, there are times when you need the exact same result every time the same network is trained on the same data. Such as for a tutorial, or perhaps operationally.  In this case you have to seed the random number generator so that you can get the same results from the same network on the same data, every time.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem demonstration\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- We'll develop a Multilayer Perceptron model to learn a short sequence of numbers increasing by 0.1 from 0.0 to 0.9. Given 0.0, the model must predict 0.1; given 0.1, the model must output 0.2; and so on.\n",
    "- We will use a network with 1 input, 10 neurons in the hidden layer, and 1 output. The network will use a mean squared error loss function and will be trained using the efficient ADAM algorithm.\n",
    "- The network needs about 1,000 epochs to solve this problem effectively, but we will only train it for 100 epochs. This is to ensure we get a model that makes errors when making predictions.\n",
    "- After the network is trained, we will make predictions on the dataset and print the mean squared error.\n",
    "- You'll see that at each run the model output different MSE values.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0003052151160642255\n",
      "MSE:  0.046978667876896515\n",
      "MSE:  0.09629765360491839\n",
      "MSE:  0.09568989718561771\n",
      "MSE:  4.03520823553351e-06\n",
      "MSE:  1.2904058471289534e-05\n",
      "MSE:  0.0020154389393818506\n",
      "MSE:  0.07920767230102693\n",
      "MSE:  0.024187434565275844\n",
      "MSE:  0.1736525164000876\n"
     ]
    }
   ],
   "source": [
    "# fit MLP to dataset and print error\n",
    "def fit_model(X, y):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=100, batch_size=len(X), verbose=0)\n",
    "    # forecast\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    print(\"MSE: \", mean_squared_error(y, yhat[:, 0]))\n",
    "\n",
    "\n",
    "# create sequence\n",
    "length = 10\n",
    "sequence = [i/float(length) for i in range(length)]\n",
    "# create X/y pairs\n",
    "df = DataFrame(sequence)\n",
    "df = concat([df.shift(1), df], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "# convert to MLP friendly format\n",
    "values = df.values\n",
    "X, y = values[:, 0], values[:, 1]\n",
    "# repeat experiment\n",
    "repeats = 10\n",
    "for _ in range(repeats):\n",
    "    fit_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- The traditional and practical way to address this problem is to run your network many times (**30+**) and use statistics to summarize the performance of your model, and compare your model to other models.\n",
    "- It is not always possible due to the very long training times of some models.\n",
    "- **This is the most robust method.**\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed the Random Number Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Another solution is to use a fixed seed for the random number generator.\n",
    "- Keras does get its source of randomness from the NumPy random number generator, so this must be seeded regardless of whether you are using a Theano or TensorFlow backend.\n",
    "- In addition, TensorFlow has its own random number generator that must also be seeded **immediately after** the NumPy random number generator\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.09197406233894682\n",
      "MSE:  0.0035952748262702947\n",
      "MSE:  0.021098260664038686\n",
      "MSE:  9.942473904910191e-06\n",
      "MSE:  0.09552536115709402\n",
      "MSE:  0.20751534552028575\n",
      "MSE:  0.026981315302370648\n",
      "MSE:  0.002129335642814133\n",
      "MSE:  0.08815277144169154\n",
      "MSE:  0.006517984376780486\n"
     ]
    }
   ],
   "source": [
    "# fit MLP to dataset and print error\n",
    "def fit_model(X, y):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=100, batch_size=len(X), verbose=0)\n",
    "    # forecast\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    print(\"MSE: \", mean_squared_error(y, yhat[:, 0]))\n",
    "\n",
    "\n",
    "# create sequence\n",
    "length = 10\n",
    "sequence = [i/float(length) for i in range(length)]\n",
    "# create X/y pairs\n",
    "df = DataFrame(sequence)\n",
    "df = concat([df.shift(1), df], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "# convert to MLP friendly format\n",
    "values = df.values\n",
    "X, y = values[:, 0], values[:, 1]\n",
    "# repeat experiment\n",
    "repeats = 10\n",
    "for _ in range(repeats):\n",
    "    fit_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold a second! I still get something different!\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- It could be that:\n",
    "    - Randomness from a Third-Party Library\n",
    "    - Randomness from Using the GPU\n",
    "    - Randomness from a Sophisticated Model\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "- https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
