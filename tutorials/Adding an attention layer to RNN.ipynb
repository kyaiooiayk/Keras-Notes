{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Generate-dataset\" data-toc-modified-id=\"Generate-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generate dataset</a></span></li><li><span><a href=\"#Vanilla-RNNS\" data-toc-modified-id=\"Vanilla-RNNS-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vanilla RNNS</a></span></li><li><span><a href=\"#Attention-layer\" data-toc-modified-id=\"Attention-layer-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Attention layer</a></span></li><li><span><a href=\"#RNN-with-attention\" data-toc-modified-id=\"RNN-with-attention-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>RNN with attention</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "**What?** Adding an attention layer to RNN\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:03:47.618845Z",
     "start_time": "2022-09-14T20:03:42.000232Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- We’ll use a very simple example of a Fibonacci sequence, where one number is constructed from previous two numbers. \n",
    "- We’ll construct the training examples from t time steps and use the value at t+1 as the target.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:03:47.629874Z",
     "start_time": "2022-09-14T20:03:47.624478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  5.  8. 13. 21. 34. 55. 89.]\n"
     ]
    }
   ],
   "source": [
    "def get_fib_seq(n, scale_data=True):\n",
    "    # Get the Fibonacci sequence\n",
    "    seq = np.zeros(n)\n",
    "    fib_n1 = 0.0\n",
    "    fib_n = 1.0\n",
    "    for i in range(n):\n",
    "        seq[i] = fib_n1 + fib_n\n",
    "        fib_n1 = fib_n\n",
    "        fib_n = seq[i]\n",
    "    scaler = []\n",
    "    if scale_data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        seq = np.reshape(seq, (n, 1))\n",
    "        seq = scaler.fit_transform(seq).flatten()\n",
    "    return seq, scaler\n",
    "\n",
    "\n",
    "fib_seq = get_fib_seq(10, False)[0]\n",
    "print(fib_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:03:47.642196Z",
     "start_time": "2022-09-14T20:03:47.632797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX =  [[[ 8.]\n",
      "  [13.]\n",
      "  [21.]]\n",
      "\n",
      " [[ 5.]\n",
      "  [ 8.]\n",
      "  [13.]]\n",
      "\n",
      " [[ 2.]\n",
      "  [ 3.]\n",
      "  [ 5.]]\n",
      "\n",
      " [[13.]\n",
      "  [21.]\n",
      "  [34.]]\n",
      "\n",
      " [[21.]\n",
      "  [34.]\n",
      "  [55.]]\n",
      "\n",
      " [[34.]\n",
      "  [55.]\n",
      "  [89.]]]\n",
      "trainY =  [ 34.  21.   8.  55.  89. 144.]\n"
     ]
    }
   ],
   "source": [
    "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
    "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)\n",
    "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[0:rows_x]\n",
    "    for i in range(time_steps-1):\n",
    "        temp = dat[i+1:rows_x+i+1]\n",
    "        X = np.column_stack((X, temp))\n",
    "    # random permutation with fixed seed\n",
    "    rand = np.random.RandomState(seed=13)\n",
    "    idx = rand.permutation(rows_x)\n",
    "    split = int(train_percent*rows_x)\n",
    "    train_ind = idx[0:split]\n",
    "    test_ind = idx[split:]\n",
    "    trainX = X[train_ind]\n",
    "    trainY = Y[train_ind]\n",
    "    testX = X[test_ind]\n",
    "    testY = Y[test_ind]\n",
    "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))\n",
    "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
    "    return trainX, trainY, testX, testY, scaler\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\n",
    "print('trainX = ', trainX)\n",
    "print('trainY = ', trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNNS\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:03:47.842554Z",
     "start_time": "2022-09-14T20:03:47.644186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 2)                 8         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 21:03:47.653892: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "time_steps = 20\n",
    "hidden_units = 2\n",
    "epochs = 30\n",
    "\n",
    "# Create a traditional RNN network\n",
    "\n",
    "\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n",
    "              activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps, 1),\n",
    "                       activation=['tanh', 'tanh'])\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:04:42.331822Z",
     "start_time": "2022-09-14T20:03:47.844852Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 2s - loss: 0.0033 - 2s/epoch - 3ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 0.0032 - 2s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 0.0031 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 2s - loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 0.0027 - 2s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 2s - loss: 0.0023 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 2s - loss: 0.0021 - 2s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 0.0019 - 2s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 0.0016 - 2s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 2s - loss: 0.0010 - 2s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 8.7533e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 7.2097e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 5.6902e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 4.4872e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 3.5934e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 2.7234e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 2.0581e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 1.5456e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 1.2696e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 2s - loss: 1.0780e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 9.9602e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 9.1544e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 8.8821e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 8.2047e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 8.5509e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 7.6452e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 8.1793e-05 - 2s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf162350a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the dataset\n",
    "trainX, trainY, testX, testY, scaler = get_fib_XY(1200, time_steps, 0.7)\n",
    "\n",
    "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:04:42.675146Z",
     "start_time": "2022-09-14T20:04:42.333844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 6.4947e-05\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.0355e-05\n",
      "Train set MSE =  6.494698027381673e-05\n",
      "Test set MSE =  2.0355368178570643e-05\n"
     ]
    }
   ],
   "source": [
    "# Evalute model\n",
    "train_mse = model_RNN.evaluate(trainX, trainY)\n",
    "test_mse = model_RNN.evaluate(testX, testY)\n",
    "\n",
    "# Print error\n",
    "print(\"Train set MSE = \", train_mse)\n",
    "print(\"Test set MSE = \", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention layer\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "\n",
    "- In Keras, we can create a custom layer that implements attention by subclassing the Layer class. \n",
    "- We need to write the `__init__` method as well as override the following methods:\n",
    "    - `build()`: Keras guide recommends adding weights in this method once the size of the inputs is known. This method ‘lazily’ creates weights. The builtin function add_weight() can be used to add weights and biases of the attention layer.\n",
    "    - `call()`: The call() method implements the mapping of inputs to outputs. It should implement the forward pass during training. We’ll implement the Bahdanau attention in our call() method.\n",
    "\n",
    "</font>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:04:42.682958Z",
     "start_time": "2022-09-14T20:04:42.677101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x, self.W)+self.b)\n",
    "        \n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        \n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with attention\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:04:42.833680Z",
     "start_time": "2022-09-14T20:04:42.688394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
      "                                                                 \n",
      " attention (attention)       (None, 2)                 22        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
    "    x = Input(shape=input_shape)\n",
    "    # return_sequences=True -> returnS hidden unit output for all previou stime steps\n",
    "    RNN_layer = SimpleRNN(\n",
    "        hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    attention_layer = attention()(RNN_layer)\n",
    "    outputs = Dense(dense_units, trainable=True,\n",
    "                    activation=activation)(attention_layer)\n",
    "    model = Model(x, outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1,\n",
    "                                            input_shape=(time_steps, 1), activation='tanh')\n",
    "model_attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:05:37.342890Z",
     "start_time": "2022-09-14T20:04:42.835508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 3s - loss: 0.0013 - 3s/epoch - 3ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 9.9490e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 9.5451e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 2s - loss: 8.9651e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 2s - loss: 8.3155e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 7.7556e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 6.9762e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 6.2822e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 5.6777e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 4.9674e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 4.4383e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 3.7803e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 3.1083e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 2.5799e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 2s - loss: 2.0671e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 1.7093e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 1.3301e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 1.1322e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 9.2838e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 8.0749e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 8.0315e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 7.6172e-05 - 2s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf16c0f8b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:05:37.649963Z",
     "start_time": "2022-09-14T20:05:37.344817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 5.8821e-05\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.2298e-06\n",
      "Train set MSE with attention =  5.882055120309815e-05\n",
      "Test set MSE with attention =  8.229778359236661e-06\n"
     ]
    }
   ],
   "source": [
    "# Evalute model\n",
    "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
    "test_mse_attn = model_attention.evaluate(testX, testY)\n",
    "\n",
    "# Print error\n",
    "print(\"Train set MSE with attention = \", train_mse_attn)\n",
    "print(\"Test set MSE with attention = \", test_mse_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- We can see that even for this simple example, the mean square error on the test set is lower with the attention layer. You can achieve better results with hyper-parameter tuning and model selection. \n",
    "    \n",
    "- This can be tried out on LSTM or an encoder decoder network as well.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T20:05:37.656156Z",
     "start_time": "2022-09-14T20:05:37.652094Z"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "- https://machinelearningmastery.com/adding-a-custom-attention-layer-to-recurrent-neural-network-in-keras/\n",
    "- https://machinelearningmastery.com/what-is-attention/\n",
    "- [Attention layer to LSTM](https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/)\n",
    "- [Attention layer to Encoder-Decoder](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/)\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
