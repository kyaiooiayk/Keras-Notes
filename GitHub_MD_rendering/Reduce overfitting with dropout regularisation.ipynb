{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Reduce overfitting with dropout regularisation\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some tips:\n",
    "    [1] Generally use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. \n",
    "    A probability too low has minimal eâ†µect and a value too high results in under-learning by the network.\n",
    "\n",
    "    [2] Use a larger network. You are likely to get better performance when dropout is used on a larger network,\n",
    "    giving the model more of an opportunity to learn independent representations.\n",
    "\n",
    "    [3] Use dropout on input (visible) as well as hidden layers. Application of dropout at each layer of the \n",
    "    network has shown good results.\n",
    "\n",
    "    [4] Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of\n",
    "    10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "\n",
    "    [5] Constrain the size of network weights. A large learning rate can result in very large network weights. \n",
    "    Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 \n",
    "    has been shown to improve results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Getting rid of the warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model withouth dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"../DATASETS/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer = \"normal\" , activation= \"relu\" ))\n",
    "    model.add(Dense(30, kernel_initializer = \"normal\" , activation= \"relu\" ))\n",
    "    model.add(Dense(1, kernel_initializer = \"normal\" , activation= \"sigmoid\" ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss= \"binary_crossentropy\" , optimizer = \"sgd\", metrics=[ \"accuracy\" ])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( \"standardize\" , StandardScaler()))\n",
    "estimators.append(( \"mlp\" , KerasClassifier(build_fn=create_baseline, epochs = 300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dropout on the Visible Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout can be applied to input neurons called the visible layer. In the example below we add a new Dropout \n",
    "layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning \n",
    "one in five inputs will be randomly excluded from each update cycle. Additionally, as recommended in the original\n",
    "paper on dropout, a constraint is imposed on\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"../DATASETS/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer = \"normal\" , activation= \"relu\" , W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer = \"normal\" , activation= \"relu\" , W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer = \"normal\" , activation= \"sigmoid\" ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss= \"binary_crossentropy\" , optimizer = \"sgd\", metrics=[ \"accuracy\" ])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( \"standardize\" , StandardScaler()))\n",
    "estimators.append(( \"mlp\" , KerasClassifier(build_fn=create_baseline, epochs = 300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We got an improvement in performance!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dropout on Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout can be applied to hidden neurons in the body of your network model. In the example below dropout is \n",
    "applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout\n",
    "rate of 20% is used as is a weight constraint on those layers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"../DATASETS/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer = \"normal\" , activation= relu , W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer = \"normal\" , activation= relu , W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer = \"normal\" , activation= sigmoid ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss = \"binary_crossentropy\" , optimizer = \"sgd\", metrics=[ \"accuracy\" ])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( \"standardize\" , StandardScaler()))\n",
    "estimators.append(( \"mlp\" , KerasClassifier(build_fn=create_baseline, epochs = 300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see that for this problem and for the chosen network configuration that using dropout in the hidden layers\n",
    "did not lift performance. In fact, performance was worse than the baseline. It is possible that additional training\n",
    "epochs are required or that further tuning is required to the learning rate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
